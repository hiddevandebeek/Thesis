---
title: "Simulation Test Thesis"
output: html_document
date: "2023-10-27"
---

---
title: "Simulation Test"
output: html_document
date: "2023-10-25"
---

## Setup
### Libraries
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(caTools)
library(plotly)
library(MASS)
library(matlib)
library(dplyr)
library(pROC)
library(parallel)
library(CalibrationCurves)
```
### Initialize variables
```{r}
# Set random seed for reproducibility
set.seed(28)
p      <- 1                               # proportion of correlated predictors
npred  <- 3                               # number of predictors
z      <- 0.2                             # z 
n      <- 1000000                         # Number of samples in the dataset
a <- 4                               # the effect size

numCores <- detectCores()
numCores
```
### Calculate sigma
```{r}
scenario <- function(){
  # set up correlations
  corr0  <-  matrix(0, npred, npred)         # matrix: set up for cov matrix, 0 on diagonals
  corr0[1:npred*p, 1:npred*p] = z            # class 0 
  diag(corr0) = 0
  # covariance structures
  sigma0 <-  diag(npred)  + corr0            # matrix: cov matrix of class 0 
  return(sigma0)
}
```


## Simulate data
Simulate data for diseased class and non-diseased class using a covariance matrix and mean vector using the MASS package. The covariance matrix is chosen such that the two predictors are correlated. The mean vector is chosen such that the diseased class has a higher mean than the non-diseased class for both predictors. The data is then combined into a single dataset.
```{r, cache=T}
# mean structures
mu0 <- c(rep(1,npred))       # vector: class 0 means

# covariance structures
sigma0 <- scenario()         # class 0

# Generate data for non-diseased class
system.time(
data <- mvrnorm(n, mu = mu0, Sigma = sigma0) %>%
  as.data.frame())

# Rename columns to "predictor1", "predictor2" ,...
colnames(data) <- c(paste0("predictor", 1:(ncol(data))))

# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
L <-  0.2*a*data$predictor1 + 0.2*a*data$predictor2 + 0.1*a*data$predictor3

L <- - mean(L) + 0.2*a*data$predictor1 + 0.2*a*data$predictor2 + 0.1*a*data$predictor3 # 50/50 split

y <- ifelse(runif(n) < plogis(L), 1, 0)
data <- cbind(data, y)

#proportion y's in data
prop.table(table(data$y))
```

## Plot distributions for Predictors
### 2D Histograms
```{r, cache=TRUE}
# Plot for Predictor 1
ggplot(data, aes(x = predictor1, fill = as.factor(y))) +
  geom_histogram(alpha = 0.5, position = 'identity', bins = 200) +
  ggtitle("Distribution of Diseased and Non-Diseased Classes for Predictor 1")

# Plot for Predictor 2
ggplot(data, aes(x = predictor2, fill = as.factor(y))) +
  geom_histogram(alpha = 0.5, position = 'identity', bins = 200) +
  ggtitle("Distribution of Diseased and Non-Diseased Classes for Predictor 2")

# Plot for Predictor 3
ggplot(data, aes(x = predictor3, fill = as.factor(y))) +
  geom_histogram(alpha = 0.5, position = 'identity', bins = 200) +
  ggtitle("Distribution of Diseased and Non-Diseased Classes for Predictor 3")
```
### 3D Histogram of Predictor 1 and Predictor 2
```{r, warning=FALSE, cache=TRUE}
# Calculate densities for class 1 and class 0
density1 <- with(data[data$y == 1, ], kde2d(predictor1, predictor2, n = 30))
density0 <- with(data[data$y == 0, ], kde2d(predictor1, predictor2, n = 30))

# Create the interactive 3D plot
p1 <- plot_ly(z = ~density1$z) %>% 
  add_surface(
    x = ~density1$x,
    y = ~density1$y,
    colorscale = list(c(0, "red"), list(1, "pink")),
    opacity = 0.8
  )

p2 <- plot_ly(z = ~density0$z) %>% 
  add_surface(
    x = ~density0$x,
    y = ~density0$y,
    colorscale = list(c(0, "blue"), list(1, "lightblue")),
    opacity = 0.8
  )

plot <- plotly::subplot(p1, p2, nrows = 1, shareX = TRUE, shareY = TRUE)
plot
```

## Logistic Regression and checking AUC for linear separability
```{r, cache=TRUE}
n_iter <- 200
sample_ratio <- 0.15

# Create a cluster with the desired number of cores (e.g., 4)
cl <- makeCluster(numCores*0.9)
on.exit(stopCluster(cl))
clusterExport(cl, varlist = c("data", "sample_ratio"))
clusterEvalQ(cl,expr= { # launch library to be used in FUN
  library(caTools)
})

# Define the function to run in each iteration
run_iteration <- function(i, data, sample_ratio) {
  sample_index <- sample(1:nrow(data), sample_ratio * nrow(data))
  sample_data <- data[sample_index, ]
  
  model <- glm(y ~ ., data = sample_data, family = binomial)
  predictions <- predict(model, newdata = sample_data, type = "response")

  # Assuming 'colAUC' is available or replaced with an equivalent function
  auc <- colAUC(predictions, sample_data$y, plotROC = FALSE)

  list(auc = auc, coefficients = model$coefficients)
}

# Run the iterations in parallel
system.time(results <- parLapply(cl, 1:n_iter, run_iteration, data, sample_ratio))

# Extract AUC values and coefficients
auc_values <- sapply(results, function(x) x$auc)
coefficients <- Reduce("+", lapply(results, function(x) x$coefficients))

# Calculate average AUC and coefficients
average_auc <- mean(auc_values)
average_coefficients <- coefficients / n_iter

print(paste("Average AUC: ", round(average_auc, 4)))
print(paste("Average coefficients: ", round(average_coefficients, 2)))

```

## Sampling function for imbalanced data
```{r}
sample_data <- function(data, size, perc_ones, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  
  # Separate data into two subsets based on y
  data_0 <- filter(data, y == 0)
  data_1 <- filter(data, y == 1)
  
  # Calculate the number of samples required from each subset
  num_ones <- round(size * perc_ones)
  num_zeros <- size - num_ones
  
  # Sample from each subset
  sampled_0 <- sample_n(data_0, min(num_zeros, nrow(data_0)))
  sampled_1 <- sample_n(data_1, min(num_ones, nrow(data_1)))
  
  # Combine the sampled subsets
  sampled_data <- rbind(sampled_0, sampled_1)
  
  # Shuffle the rows
  sampled_data <- sampled_data[sample(nrow(sampled_data)), ]
  
  return(sampled_data)
}
```


## Plotting ROC curves
### Function to plot ROC curves
```{r, warning=FALSE, message=FALSE}
fit_and_evaluate <- function(original_data, times, sample_size, perc_ones, test_size = 0.2, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  
  auc_values <- numeric(times)  # Vector to store AUC values
  roc_curves <- data.frame()    # Data frame to store ROC curves points
  
  for (i in 1:times) {
    # Sample from the original dataset
    data <- sample_data(original_data, size = sample_size, perc_ones = perc_ones, seed = seed)
    
    # Split the data
    index <- sample(1:nrow(data), size = round(nrow(data) * test_size))
    test_set <- data[index, ]
    train_set <- data[-index, ]
    
    # Fit logistic regression model
    full_model <- glm(y ~ (.)^2, data = train_set, family = "binomial")
    step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
    
    # Predict on test set
    predictions <- predict(step_model, newdata = test_set, type = "response")
    
    # Evaluate model performance using AUC
    roc_obj <- suppressMessages(roc(response = test_set$y, predictor = predictions, quiet = TRUE))
    auc_values[i] <- auc(roc_obj)
    
    # Store ROC curve points
    roc_curves <- rbind(roc_curves, data.frame(
      tpr = roc_obj$sensitivities,
      fpr = 1 - roc_obj$specificities,
      iteration = i
    ))
  }
  
  # Calculate the average AUC
  mean_auc <- mean(auc_values)
  
  # Plot all ROC curves
  roc_plot <- ggplot(roc_curves, aes(x = fpr, y = tpr, group = iteration, color = as.factor(iteration))) +
              geom_line(alpha = 0.3) +
              geom_line(data = data.frame(fpr = c(0, 1), tpr = c(0, 1)),
                        aes(x = fpr, y = tpr), linetype = "dashed", inherit.aes = FALSE) +
              theme_minimal() +
              labs(color = "Iteration") +
              ggtitle("ROC Curves from Multiple Iterations")
  
  return(list(AverageAUC = mean_auc, ROCPlot = roc_plot))
}
```

### Function with different sample sizes and 50% of ones
```{r, warning=FALSE, message=FALSE}
# Example usage
results.5.100 <- fit_and_evaluate(data, times = 40, sample_size = 100, test_size = .8, perc_ones = 0.5)
results.5.200 <- fit_and_evaluate(data, times = 40, sample_size = 200, test_size = .8, perc_ones = 0.5)
results.5.400 <- fit_and_evaluate(data, times = 40, sample_size = 400, test_size = .8, perc_ones = 0.5)

# Print average AUC
print(results.5.100$AverageAUC)
print(results.5.200$AverageAUC)
print(results.5.400$AverageAUC)

# Plot ROC curves
print(results.5.100$ROCPlot)
print(results.5.200$ROCPlot)
print(results.5.400$ROCPlot)
```

### Function with different sample sizes and 20% of ones
```{r, warning=FALSE, message=FALSE}
# Example usage
results.20.100 <- fit_and_evaluate(data, times = 40, sample_size = 100, test_size = .8, perc_ones = 0.2)
results.20.200 <- fit_and_evaluate(data, times = 40, sample_size = 200, test_size = .8, perc_ones = 0.2)
results.20.400 <- fit_and_evaluate(data, times = 40, sample_size = 400, test_size = .8, perc_ones = 0.2)

# Print average AUC
print(results.20.100$AverageAUC)
print(results.20.200$AverageAUC)
print(results.20.400$AverageAUC)

# Plot ROC curves
print(results.20.100$ROCPlot)
print(results.20.200$ROCPlot)
print(results.20.400$ROCPlot)
```

### Function with different sample sizes and 5% of ones
```{r, warning=FALSE, message=FALSE}
# Example usage
results.05.100 <- fit_and_evaluate(data, times = 40, sample_size = 400, test_size = .6, perc_ones = 0.05)
results.05.200 <- fit_and_evaluate(data, times = 40, sample_size = 800, test_size = .6, perc_ones = 0.05)
results.05.400 <- fit_and_evaluate(data, times = 40, sample_size = 1600, test_size = .6, perc_ones = 0.05)

# Print average AUC
print(results.05.100$AverageAUC)
print(results.05.200$AverageAUC)
print(results.05.400$AverageAUC)

# Plot ROC curves
print(results.05.100$ROCPlot)
print(results.05.200$ROCPlot)
print(results.05.400$ROCPlot)

```

### Example of calibration plot
```{r}
experiment.data <-sample_data(data, 1000, 0.5, seed = 28)
test_set <- experiment.data[1:500, ]
train_set <- experiment.data[501:1000, ]

full_model <- glm(y ~ (.)^2, data = train_set, family = "binomial")
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)

pHat = predict(step_model, newdata = test_set, type = "response")
yTest = test_set$y

invisible(val.prob.ci.2(pHat, yTest, logistic.cal = TRUE, smooth = "none"))
```











## Experiment 1: Percentage of selecting the correct model
```{r}

```

## Experiment 2: Percentage of selecting the best performing validated model
```{r}

```

