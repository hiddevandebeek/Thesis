---
title: "Visualization Report"
author: "Hidde van de Beek"
date: "2023-11-20"
output: html_document
---


# Analysis

## Libraries
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(caTools)
library(plotly)
library(MASS)
library(matlib)
library(pROC)
library(parallel)
library(CalibrationCurves)
library(caret)
library(wesanderson)
library(tidyverse)
```

## Open data
```{r}
data <- readRDS("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/data/data_report.rds")
```

## Functions
```{r}
sample_data <- function(data, size, perc_ones) {
  
  # Separate data into two subsets based on y
  data_0 <- filter(data, y == 0)
  data_1 <- filter(data, y == 1)
  
  # Calculate the number of samples required from each subset
  num_ones <- round(size * perc_ones)
  num_zeros <- size - num_ones
  
  # Sample from each subset
  sampled_0 <- sample_n(data_0, min(num_zeros, nrow(data_0)))
  sampled_1 <- sample_n(data_1, min(num_ones, nrow(data_1)))
  
  # Combine the sampled subsets
  sampled_data <- rbind(sampled_0, sampled_1)
  
  # Shuffle the rows
  sampled_data <- sampled_data[sample(nrow(sampled_data)), ]
  
  return(sampled_data)
}

calculate_optimism_corrected_auc <- function(sampled_data, model_formula, n_bootstrap) {
    naive_auc <- auc(roc(sampled_data$y, predict(glm(model_formula, data = sampled_data, family = "binomial"), type = "response")))
    optimism_values <- numeric(n_bootstrap)

    for (i in 1:n_bootstrap) {
      bootstrap_sample <- sample_data(sampled_data, nrow(sampled_data), perc_ones)
      fitted_model <- glm(model_formula, data = bootstrap_sample, family = "binomial")
      
      # AUC on bootstrap sample
      auc_bootstrap <- auc(roc(bootstrap_sample$y, predict(fitted_model, newdata = bootstrap_sample, type = "response")))
      
      # AUC on original sampled data
      auc_original <- auc(roc(sampled_data$y, predict(fitted_model, newdata = sampled_data, type = "response")))
      
      optimism_values[i] <- auc_bootstrap - auc_original
    }

    naive_auc - mean(optimism_values)
  }
```


## Experiment: Percentage of selecting the correct model
### in parallel
#### Function
```{r}
compare_models_parallel <- function(data, n_iter, samplesize, perc_ones, n_bootstrap) {
  # Function to be run in each parallel process
  run_iteration <- function(i) {
    sampled_data <- sample_data(data, samplesize, perc_ones)
    sampled_data$y <- factor(sampled_data$y, levels = c(0, 1))

    model1_formula <- y ~ predictor1 + predictor2 + predictor3
    model2_formula <- y ~ predictor1 + predictor2

    model1 <- glm(model1_formula, data=sampled_data, family="binomial")
    model2 <- glm(model2_formula, data=sampled_data, family="binomial")

    aic_winner <- ifelse(AIC(model1) < AIC(model2), 1, 2)
    bic_winner <- ifelse(BIC(model1) < BIC(model2), 1, 2)
    auc_winner <- ifelse(calculate_optimism_corrected_auc(sampled_data, model1_formula, n_bootstrap) > calculate_optimism_corrected_auc(sampled_data, model2_formula, n_bootstrap), 1, 2)

    return(c(aic_winner, bic_winner, auc_winner))
  }

  # Determine the number of cores and split the work
  no_cores <- detectCores() - 3  # Optimal number of cores
  results <- mclapply(1:n_iter, run_iteration, mc.cores = no_cores)

   # Process results
  results_matrix <- matrix(unlist(results), ncol = 3, byrow = TRUE)
  model1_aic_count <- sum(results_matrix[, 1] == 1)
  model1_bic_count <- sum(results_matrix[, 2] == 1)
  model1_auc_count <- sum(results_matrix[, 3] == 1)

  # Calculate percentages
  calculate_percentage <- function(count) (count / n_iter) * 100
  model1_aic_percent <- calculate_percentage(model1_aic_count)
  model1_bic_percent <- calculate_percentage(model1_bic_count)
  model1_auc_percent <- calculate_percentage(model1_auc_count)
  model2_aic_percent <- 100 - model1_aic_percent
  model2_bic_percent <- 100 - model1_bic_percent
  model2_auc_percent <- 100 - model1_auc_percent

  # Create and return a table of the results
  results_table <- data.frame(
    Model = c("Model 1", "Model 2"),
    AIC_Percentage = c(model1_aic_percent, model2_aic_percent),
    BIC_Percentage = c(model1_bic_percent, model2_bic_percent),
    AUC_Percentage = c(model1_auc_percent, model2_auc_percent)
  )

  return(results_table)
}
```

#### Run
```{r, message=FALSE, warning=FALSE, cache=T}
results_list.05 <- list()
results_list.2 <- list()
results_list.5 <- list()

for (size in seq(50, 1000, by = 50)) {
  results_list.05[[paste("compare.05", size, sep = ".")]] <- compare_models_parallel(data, 2500, size, 0.05, 80)
}

for (size in seq(50, 1000, by = 50)) {
  results_list.2[[paste("compare.2", size, sep = ".")]] <- compare_models_parallel(data, 2500, size, 0.2, 80)
}

for (size in seq(50, 1000, by = 50)) {
  results_list.5[[paste("compare.5", size, sep = ".")]] <- compare_models_parallel(data, 200, size, 0.5, 80)
}
```

### plotting 
#### function
```{r}
create_minimalistic_plot <- function(results_list, title_suffix = "") {
  # Combine all results into a single data frame
  combined_results <- bind_rows(results_list, .id = "Sample_Size")

  # Convert from wide to long format
  combined_results_long <- combined_results %>%
    mutate(Sample_Size = as.numeric(gsub("compare.*\\.", "", Sample_Size))) %>%
    pivot_longer(cols = c("AIC_Percentage", "BIC_Percentage", "AUC_Percentage"), names_to = "Criterion", values_to = "Percentage") %>%
    filter(Model == "Model 1") %>%
    mutate(Criterion = factor(Criterion, levels = c("AIC_Percentage", "BIC_Percentage", "AUC_Percentage"), labels = c("AIC", "BIC", "AUC (optimism adjusted)")))

  # Create a minimalistic plot
  plot_minimalistic <- ggplot(combined_results_long, aes(x = Sample_Size, y = Percentage, color = Criterion)) +
    geom_line(linewidth = 1) +  # Solid lines
    scale_color_manual(values = wes_palette("Darjeeling1")) +  # Wes Anderson palette
    theme_minimal(base_size = 14) +  # Minimalistic theme
    theme(legend.position = "bottom",  # Move legend to bottom
          legend.title = element_blank(),  # Remove the legend title
          plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),  # Bold and centered title
          axis.text = element_text(color = "black"),  # Black axis text
          axis.title = element_text(size = 14, face = "bold"),  # Bold axis titles
          panel.grid.minor = element_blank(),  # No minor grid lines
          panel.border = element_blank()) +  # No border
    labs(title = paste("Effect of Sample Size on Model 1", title_suffix),
         x = "Sample Size",
         y = "Percentage (%)")

  return(plot_minimalistic)
}
```

#### all datasets
```{r}
plot.05 <- create_minimalistic_plot(results_list.05, "with 5% Event Occurrence")
plot.2 <- create_minimalistic_plot(results_list.2, "with 20% Event Occurrence")
plot.5 <- create_minimalistic_plot(results_list.5, "with 50% Event Occurrence")


plot.05
plot.2
plot.5
```

#### saving
```{r}
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 1/plot.05.png", plot.05, width = 8, height = 6, units = "in", dpi = 1000)
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 1/plot.2.png", plot.2, width = 8, height = 6, units = "in", dpi = 1000)
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 1/plot.5.png", plot.5, width = 8, height = 6, units = "in", dpi = 1000)
```


## Calibration
```{r}
set.seed(123)
# Set up the number of iterations and the percentage of ones for sampling
n_iterations <- 1000
perc_ones <- 0.05 # Adjust this as per your requirement

# Placeholder to store the results of each iteration
iteration_results <- list()

# Loop through the iterations to train models and collect predictions
for(i in 1:n_iterations) {

  train_data <- sample_data(data, size = 200, perc_ones)
  test_data <- sample_data(data, size = 2000, perc_ones)
  
  # Fit logistic regression model
  model <- glm(y ~ ., data = train_data, family = "binomial")
  
  # Get predicted probabilities
  test_data$predicted_prob <- predict(model, newdata = test_data, type = "response")
  
  # Bin these predicted probabilities
  test_data$prob_bin <- cut(test_data$predicted_prob, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)
  
  # Store the iteration data with an identifier for iteration
  test_data$iteration <- i
  iteration_results[[i]] <- test_data
}

# Combine all iterations into one dataframe
all_iterations <- bind_rows(iteration_results)

# Aggregate data to obtain mean predicted probability and observed outcome per bin
aggregated_data <- all_iterations %>%
  group_by(prob_bin, iteration) %>%
  summarise(
    mean_pred = mean(predicted_prob),
    observed = mean(y),
    .groups = 'drop'
  )


# Plot the combined results with an adjusted loess smoother
ggplot(aggregated_data, aes(x = mean_pred, y = observed, group = iteration)) +
    geom_abline(slope = 1, intercept = 0, linewidth = 1, color = "darkgray") +
    geom_smooth(aes(x = mean_pred, y = observed, group = iteration), 
                method = "loess", formula = y ~ x,
                se = F, color = "#101011", alpha = 0.01,
                linewidth = 0.01,
                span = 1,  # Increased span for smoother curve
                method.args = list(degree = 1)) +  # Reduce the degree of the polynomials used
    theme_minimal() +
    labs(x = "Mean Predicted Probability", y = "Observed Outcome") +
    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    ggtitle("Calibration Plot Over Multiple Iterations")
```




