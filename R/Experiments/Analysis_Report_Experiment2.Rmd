---
title: "Visualization Report"
author: "Hidde van de Beek"
date: "2023-11-20"
output: html_document
---


# Analysis

## Libraries
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(caTools)
library(plotly)
library(MASS)
library(matlib)
library(pROC)
library(parallel)
library(CalibrationCurves)
library(caret)
library(wesanderson)
```

## Open data
```{r}
data <- readRDS("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/data/data_report.rds")
```

## Function for imbalanced data
```{r}
sample_data <- function(data, size, perc_ones, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  
  # Separate data into two subsets based on y
  data_0 <- filter(data, y == 0)
  data_1 <- filter(data, y == 1)
  
  # Calculate the number of samples required from each subset
  num_ones <- round(size * perc_ones)
  num_zeros <- size - num_ones
  
  # Sample from each subset
  sampled_0 <- sample_n(data_0, min(num_zeros, nrow(data_0)))
  sampled_1 <- sample_n(data_1, min(num_ones, nrow(data_1)))
  
  # Combine the sampled subsets
  sampled_data <- rbind(sampled_0, sampled_1)
  
  # Shuffle the rows
  sampled_data <- sampled_data[sample(nrow(sampled_data)), ]
  
  return(sampled_data)
}
```

## Determine correct model based on cross validation
```{r}

```



## Experiment: Percentage of selecting the correct model
### in series
```{r}
compare_models <- function(data, n_iter, samplesize, perc_ones, n_bootstrap) {
  model1_aic_count <- 0
  model1_bic_count <- 0
  model1_auc_count <- 0

  calculate_optimism_corrected_auc <- function(sampled_data, model_formula) {
    naive_auc <- auc(roc(sampled_data$y, predict(glm(model_formula, data = sampled_data, family = "binomial"), type = "response")))
    optimism_values <- numeric(n_bootstrap)

    for (i in 1:n_bootstrap) {
      bootstrap_sample <- sample_data(sampled_data, nrow(sampled_data), perc_ones)
      fitted_model <- glm(model_formula, data = bootstrap_sample, family = "binomial")
      
      # AUC on bootstrap sample
      auc_bootstrap <- auc(roc(bootstrap_sample$y, predict(fitted_model, newdata = bootstrap_sample, type = "response")))
      
      # AUC on original sampled data
      auc_original <- auc(roc(sampled_data$y, predict(fitted_model, newdata = sampled_data, type = "response")))
      
      optimism_values[i] <- auc_bootstrap - auc_original
    }

    naive_auc - mean(optimism_values)
  }

  for (i in 1:n_iter) {
    # Sample the data using sample_data function
    sampled_data <- sample_data(data, samplesize, perc_ones)
    sampled_data$y <- factor(sampled_data$y, levels = c(0, 1))

    # Fit Model 1 and Model 2
    model1_formula <- y ~ predictor1 + predictor2 + predictor3
    model2_formula <- y ~ predictor1 + predictor2

    model1 <- glm(model1_formula, data=sampled_data, family="binomial")
    model2 <- glm(model2_formula, data=sampled_data, family="binomial")

    # Compare AIC and BIC
    model1_aic_count <- model1_aic_count + (AIC(model1) < AIC(model2))
    model1_bic_count <- model1_bic_count + (BIC(model1) < BIC(model2))

    # Calculate optimism-corrected AUC for each model
    auc_model1 <- calculate_optimism_corrected_auc(sampled_data, model1_formula)
    auc_model2 <- calculate_optimism_corrected_auc(sampled_data, model2_formula)

    # Compare AUC
    model1_auc_count <- model1_auc_count + (auc_model1 > auc_model2)
  }

  # Calculate percentages
  calculate_percentage <- function(count) (count / n_iter) * 100
  model1_aic_percent <- calculate_percentage(model1_aic_count)
  model1_bic_percent <- calculate_percentage(model1_bic_count)
  model1_auc_percent <- calculate_percentage(model1_auc_count)

  # Create and return a table of the results
  results_table <- data.frame(
    Model = c("Model 1", "Model 2"),
    AIC_Percentage = c(model1_aic_percent, 100 - model1_aic_percent),
    BIC_Percentage = c(model1_bic_percent, 100 - model1_bic_percent),
    AUC_Percentage = c(model1_auc_percent, 100 - model1_auc_percent)
  )
  return(results_table)
}
```

### in parallel
```{r}
compare_models_parallel <- function(data, n_iter, samplesize, perc_ones, n_bootstrap) {
  
  calculate_optimism_corrected_auc <- function(sampled_data, model_formula) {
    naive_auc <- auc(roc(sampled_data$y, predict(glm(model_formula, data = sampled_data, family = "binomial"), type = "response")))
    optimism_values <- numeric(n_bootstrap)

    for (i in 1:n_bootstrap) {
      bootstrap_sample <- sample_data(sampled_data, nrow(sampled_data), perc_ones)
      fitted_model <- glm(model_formula, data = bootstrap_sample, family = "binomial")
      
      # AUC on bootstrap sample
      auc_bootstrap <- auc(roc(bootstrap_sample$y, predict(fitted_model, newdata = bootstrap_sample, type = "response")))
      
      # AUC on original sampled data
      auc_original <- auc(roc(sampled_data$y, predict(fitted_model, newdata = sampled_data, type = "response")))
      
      optimism_values[i] <- auc_bootstrap - auc_original
    }

    naive_auc - mean(optimism_values)
  }

  # Function to be run in each parallel process
  run_iteration <- function(i) {
    sampled_data <- sample_data(data, samplesize, perc_ones)
    sampled_data$y <- factor(sampled_data$y, levels = c(0, 1))

    model1_formula <- y ~ predictor1 + predictor2 + predictor3
    model2_formula <- y ~ predictor1 + predictor2

    model1 <- glm(model1_formula, data=sampled_data, family="binomial")
    model2 <- glm(model2_formula, data=sampled_data, family="binomial")

    aic_winner <- ifelse(AIC(model1) < AIC(model2), 1, 2)
    bic_winner <- ifelse(BIC(model1) < BIC(model2), 1, 2)
    auc_winner <- ifelse(calculate_optimism_corrected_auc(sampled_data, model1_formula) > calculate_optimism_corrected_auc(sampled_data, model2_formula), 1, 2)

    return(c(aic_winner, bic_winner, auc_winner))
  }

  # Determine the number of cores and split the work
  no_cores <- detectCores() - 1  # Save one core for system stability
  results <- mclapply(1:n_iter, run_iteration, mc.cores = no_cores)

   # Process results
  results_matrix <- matrix(unlist(results), ncol = 3, byrow = TRUE)
  model1_aic_count <- sum(results_matrix[, 1] == 1)
  model1_bic_count <- sum(results_matrix[, 2] == 1)
  model1_auc_count <- sum(results_matrix[, 3] == 1)

  # Calculate percentages
  calculate_percentage <- function(count) (count / n_iter) * 100
  model1_aic_percent <- calculate_percentage(model1_aic_count)
  model1_bic_percent <- calculate_percentage(model1_bic_count)
  model1_auc_percent <- calculate_percentage(model1_auc_count)
  model2_aic_percent <- 100 - model1_aic_percent
  model2_bic_percent <- 100 - model1_bic_percent
  model2_auc_percent <- 100 - model1_auc_percent

  # Create and return a table of the results
  results_table <- data.frame(
    Model = c("Model 1", "Model 2"),
    AIC_Percentage = c(model1_aic_percent, model2_aic_percent),
    BIC_Percentage = c(model1_bic_percent, model2_bic_percent),
    AUC_Percentage = c(model1_auc_percent, model2_auc_percent)
  )

  return(results_table)
}
```

```{r, message=FALSE, warning=FALSE, cache=T}
results_list.05 <- list()
results_list.2 <- list()
results_list.5 <- list()

for (size in seq(50, 1000, by = 50)) {
  results_list.05[[paste("compare.05", size, sep = ".")]] <- compare_models_parallel(data, 500, size, 0.05, 40)
}

for (size in seq(50, 1000, by = 50)) {
  results_list.2[[paste("compare.2", size, sep = ".")]] <- compare_models_parallel(data, 500, size, 0.2, 40)
}

for (size in seq(50, 1000, by = 50)) {
  results_list.5[[paste("compare.5", size, sep = ".")]] <- compare_models_parallel(data, 500, size, 0.5, 40)
}
```

### plotting 
#### function
```{r}
create_minimalistic_plot <- function(results_list, title_suffix = "") {
  # Combine all results into a single data frame
  combined_results <- bind_rows(results_list, .id = "Sample_Size")

  # Convert from wide to long format
  combined_results_long <- combined_results %>%
    mutate(Sample_Size = as.numeric(gsub("compare.*\\.", "", Sample_Size))) %>%
    pivot_longer(cols = c("AIC_Percentage", "BIC_Percentage", "AUC_Percentage"), names_to = "Criterion", values_to = "Percentage") %>%
    filter(Model == "Model 1") %>%
    mutate(Criterion = factor(Criterion, levels = c("AIC_Percentage", "BIC_Percentage", "AUC_Percentage"), labels = c("AIC", "BIC", "AUC (optimism adjusted)")))

  # Create a minimalistic plot
  plot_minimalistic <- ggplot(combined_results_long, aes(x = Sample_Size, y = Percentage, color = Criterion)) +
    geom_line(size = 1) +  # Solid lines
    scale_color_manual(values = wes_palette("Darjeeling1")) +  # Wes Anderson palette
    theme_minimal(base_size = 14) +  # Minimalistic theme
    theme(legend.position = "bottom",  # Move legend to bottom
          legend.title = element_blank(),  # Remove the legend title
          plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),  # Bold and centered title
          axis.text = element_text(color = "black"),  # Black axis text
          axis.title = element_text(size = 14, face = "bold"),  # Bold axis titles
          panel.grid.minor = element_blank(),  # No minor grid lines
          panel.border = element_blank()) +  # No border
    labs(title = paste("Effect of Sample Size on Model 1", title_suffix),
         x = "Sample Size",
         y = "Percentage (%)")

  return(plot_minimalistic)
}
```

#### all datasets
```{r}
plot.05 <- create_minimalistic_plot(results_list.05, "with 5% Event Occurrence")
plot.2 <- create_minimalistic_plot(results_list.2, "with 20% Event Occurrence")
plot.5 <- create_minimalistic_plot(results_list.5, "with 50% Event Occurrence")
```


```{r}
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 2/plot.05.png", plot.05, width = 8, height = 6, units = "in", dpi = 1000)
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 2/plot.2.png", plot.2, width = 8, height = 6, units = "in", dpi = 1000)
ggsave("/Users/hiddevandebeek/Documents/Documents - Hidde’s MacBook Pro/Github/Thesis/results/experiment 2/plot.5.png", plot.5, width = 8, height = 6, units = "in", dpi = 1000)
```



